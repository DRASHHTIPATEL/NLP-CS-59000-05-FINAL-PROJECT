# -*- coding: utf-8 -*-
"""Bert_Score_qa _silver_2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16K8paMleqYCh7oAqL3MVkeV0hTMStZeZ
"""

!pip install langchain

import os
os.environ["HF_TOKEN"]= "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"

!pip install transformers
!pip install bert-score

!pip install datasets

"""#SILVER DATASET"""

from transformers import pipeline
import json
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Load the silver dataset
with open("silver_dataset_nlp.json", "r") as f:
    silver_data = json.load(f)

# Extract contexts
contexts = [item["context"] for item in silver_data["contexts"]]

# Load pre-trained pipelines
question_generation_pipeline = pipeline("text2text-generation", model="Drashtip/fine_tuned_t5_squad")
question_answering_pipeline = pipeline("question-answering", model="Drashtip/fine_tuned_t5_squad")
sentence_embedder = SentenceTransformer("all-MiniLM-L6-v2")  # For question deduplication

# Generate questions and answers
generated_data = []

for context in contexts:
    # Generate diverse questions with sampling
    question_gen_input = f"generate questions: {context}"
    questions = question_generation_pipeline(
        question_gen_input,
        max_length=128,
        num_return_sequences=10,  # Generate more questions
        do_sample=True,          # Enable sampling
        top_p=0.95,              # Top-p (nucleus) sampling
        temperature=0.7          # Temperature for diversity
    )

    # Prepare unique question-answer pairs
    qa_pairs = []
    unique_questions = set()
    for question in questions:
        question_text = question["generated_text"].strip()

        # Check for duplicate questions using cosine similarity
        if question_text in unique_questions:
            continue
        unique_questions.add(question_text)

        # Generate answers
        answer = question_answering_pipeline(question=question_text, context=context)
        answer_text = answer["answer"].strip() if "answer" in answer else "no answer"

        # Validate answer relevance (non-empty, meaningful)
        if answer_text and answer_text.lower() not in ["", "no answer"]:
            qa_pairs.append({"question": question_text, "answer": answer_text})

    # Add context and QA pairs to the dataset
    generated_data.append({"context": context, "qa_pairs": qa_pairs})

# Save the improved dataset
with open("improved_qa_output.json", "w") as outfile:
    json.dump(generated_data, outfile, indent=4)

print("Improved questions and answers saved to 'improved_qa_output.json'")

from transformers import pipeline
from bert_score import score
import json

# Load the silver dataset
with open("silver_dataset_nlp.json", "r") as f:
    silver_data = json.load(f)

# Extract contexts and QA pairs
contexts = [item["context"] for item in silver_data["contexts"]]
qa_pairs_list = [item["qa_pairs"] for item in silver_data["contexts"]]

# Load pre-trained QA pipeline
qa_pipeline = pipeline("question-answering", model="Drashtip/fine_tuned_t5_squad")

# Initialize lists to collect scores
precision_scores = []
recall_scores = []
f1_scores = []

# Function to compute BERTScore for a single context and its QA pairs
def compute_bertscore(context, qa_pairs):
    reference_answers = []
    generated_answers = []

    for qa_pair in qa_pairs:
        question = qa_pair["question"]
        expected_answer = qa_pair["answer"]

        # Use the model to predict the answer
        result = qa_pipeline(question=question, context=context)
        predicted_answer = result["answer"] if "answer" in result else "no answer"

        # Add to lists for BERTScore evaluation
        reference_answers.append(expected_answer.strip())
        generated_answers.append(predicted_answer.strip())

    # Compute BERTScore
    P, R, F1 = score(generated_answers, reference_answers, lang="en")

    # Return scores
    return P.tolist(), R.tolist(), F1.tolist()

# Evaluate all contexts and their QA pairs
for context, qa_pairs in zip(contexts, qa_pairs_list):
    # Skip if no QA pairs
    if not qa_pairs:
        continue

    # Compute BERTScore for this context
    P, R, F1 = compute_bertscore(context, qa_pairs)

    # Collect scores
    precision_scores.extend(P)
    recall_scores.extend(R)
    f1_scores.extend(F1)

# Compute average BERTScore metrics
average_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0.0
average_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0.0
average_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0

# Print results
print(f"Average BERTScore Precision: {average_precision:.4f}")
print(f"Average BERTScore Recall: {average_recall:.4f}")
print(f"Average BERTScore F1: {average_f1:.4f}")

"""#BERT SILVER EVALUATION"""

from transformers import pipeline
from bert_score import score
import json

# Load the silver dataset
with open("silver_dataset_nlp.json", "r") as f:
    silver_data = json.load(f)

# Extract contexts and QA pairs
contexts = [item["context"] for item in silver_data["contexts"]]
qa_pairs_list = [item["qa_pairs"] for item in silver_data["contexts"]]

# Load pre-trained QA pipeline
qa_pipeline = pipeline("question-answering", model="Drashtip/llama-finetuned")

# Initialize lists to collect scores
precision_scores = []
recall_scores = []
f1_scores = []

# Function to compute BERTScore for a single context and its QA pairs
def compute_bertscore(context, qa_pairs):
    reference_answers = []
    generated_answers = []

    for qa_pair in qa_pairs:
        question = qa_pair["question"]
        expected_answer = qa_pair["answer"]

        # Use the model to predict the answer
        result = qa_pipeline(question=question, context=context)
        predicted_answer = result["answer"] if "answer" in result else "no answer"

        # Add to lists for BERTScore evaluation
        reference_answers.append(expected_answer.strip())
        generated_answers.append(predicted_answer.strip())

    # Compute BERTScore
    P, R, F1 = score(generated_answers, reference_answers, lang="en")

    # Return scores
    return P.tolist(), R.tolist(), F1.tolist()

# Evaluate all contexts and their QA pairs
for context, qa_pairs in zip(contexts, qa_pairs_list):
    # Skip if no QA pairs
    if not qa_pairs:
        continue

    # Compute BERTScore for this context
    P, R, F1 = compute_bertscore(context, qa_pairs)

    # Collect scores
    precision_scores.extend(P)
    recall_scores.extend(R)
    f1_scores.extend(F1)

# Compute average BERTScore metrics
average_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0.0
average_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0.0
average_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0

# Print results
print(f"Average BERTScore Precision: {average_precision:.4f}")
print(f"Average BERTScore Recall: {average_recall:.4f}")
print(f"Average BERTScore F1: {average_f1:.4f}")









